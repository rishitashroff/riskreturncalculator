# -*- coding: utf-8 -*-
"""Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bZERR4t8Xm3xSN7HwI3wyN6ele43ZMAo
"""

# importing required libraries
import numpy as np
import pandas as pd

import datetime as dt
import yfinance as yf
import pandas_datareader.data as web
import cpi
import wbdata
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, LSTM, Dropout, LeakyReLU
from sklearn.metrics import mean_squared_error


import matplotlib.pyplot as plt


cpi.update()

# importing sp500 list of stocks
sp500_stocks = pd.read_csv("/content/drive/MyDrive/DS4420/Project/sp500_stocks.csv", encoding="latin-1")
#print(sp500_stocks)

# customize seed for different stocks
#np.random.seed(42)

# list of all potential stocks
#ticker_list = list(sp500_stocks["Ticker"])

# using ticker list as stock list coz want to run idea by calvin
stock_list = list(sp500_stocks["Ticker"])

# selecting 15 random stocks
#stock_list = np.random.choice(ticker_list,15,replace=False)
#stock_list = list(np.sort(stock_list))
#print(f'These are the fifteen stocks assigned to you: {" ".join(stock_list)}')

# defining start and end date
start = dt.datetime(2000, 1, 1)
end = dt.datetime(2024, 12, 31)

# # customize seed for different stocks
# np.random.seed(42)

# # list of all potential stocks
# ticker_list = ['AAPL','AXP', 'BAC', 'C', 'CSCO', 'GS', 'IBM', 'INTC', 'JPM', 'MSFT', 'NVDA', 'CRM', 'QCOM', 'NOW', 'ORCL', 'AVGO', 'GDDY',
#                'WIX', 'TSM', 'TSLA', 'SNOW', 'HUBS', 'DOCU', 'MS']

# # selecting 15 random stocks
# stock_list = np.random.choice(ticker_list,15,replace=False)
# stock_list = list(np.sort(stock_list))
# print(f'These are the fifteen stocks assigned to you: {" ".join(stock_list)}')

# # defining start and end date
# start = dt.datetime(2000, 1, 1)
# end = dt.datetime(2024, 12, 31)

# # getting returns of the stocks
# # calculates returns using adjusted close price
# # ensures that we have returns of only those dates where all companies have available information
# returns = yf.download(stock_list, start-pd.offsets.BDay(1), end+pd.offsets.BDay(1), auto_adjust=False)['Adj Close'].pct_change().dropna()

# # gets all returns Close, high, low, open, volume
# #returns = yf.download(stock_list, start, end)

# # S&P 500
# sp500 = yf.Ticker("^GSPC")
# sp500_data = sp500.history(period="25y")
# sp500_data['SP500'] = sp500_data['Close'].pct_change()
# sp500_data = sp500_data.drop(columns=["Open", "High", "Low", "Close", "Volume", "Dividends", "Stock Splits"])

# sp500_data.index = sp500_data.index.strftime('%m-%d-%Y')
# sp500_data.index = pd.to_datetime(sp500_data.index)
# # sp500_mon = sp500_data.resample(rule = 'ME').apply(lambda x: x.add(1).prod().sub(1))

# getting prices of the stocks
prices = yf.download(stock_list, start-pd.offsets.BDay(1), end+pd.offsets.BDay(1), auto_adjust=False)['Adj Close']

# defining empty list
filtered_stocks = []

for i in prices.columns:
    # getting stocks who had some prices in 2005 jan
    if prices.loc[pd.Timestamp("2005-01-03"), i] >= 0:

        # adding them to filtered list
        filtered_stocks.append(i)

# getting df
filtered_df = prices[filtered_stocks]
#print(filtered_df)

# dropping null values
filtered_df = filtered_df.dropna()

# getting daily returns
returns = filtered_df.pct_change().dropna()
#returns

# getting a sector analysis
sp500_stocks[sp500_stocks["Ticker"].isin(stock_list)]["Sector"].value_counts(normalize=True)

# S&P 500
sp500 = yf.Ticker("^GSPC")
sp500_data = sp500.history(period="21y")
sp500_data['SP500'] = sp500_data['Close'].pct_change()
sp500_data = sp500_data.drop(columns=["Open", "High", "Low", "Close", "Volume", "Dividends", "Stock Splits"])

sp500_data.index = sp500_data.index.strftime('%m-%d-%Y')
sp500_data.index = pd.to_datetime(sp500_data.index)
# sp500_mon = sp500_data.resample(rule = 'ME').apply(lambda x: x.add(1).prod().sub(1))

#sp500_data

# Find finding monthly returns
returns = pd.merge(returns, sp500_data, left_index = True, right_index = True )

returns_mon = returns.resample(rule = 'ME').apply(lambda x: x.add(1).prod().sub(1))

returns_mon

# Standard deviation for 2018, May SP500
sp500_stdev = returns.loc[(returns.index.year == 2018) & (returns.index.month == 5)]['SP500'].std()
print(f"The standard deviation for SP500 in May of 2018 is {sp500_stdev}")

# Calculating STDEV for each month
monthly_std = returns.groupby(by=[returns.index.month, returns.index.year]).std()
monthly_std.index = pd.to_datetime(monthly_std.index.map(lambda x: f"{x[1]}-{x[0]}"), format="%Y-%m")
monthly_std.sort_index(inplace=True)
monthly_std.index = returns_mon.index

# Double check
monthly_std

# Finding other features we care about
# Finding risk free-rate
rf = web.DataReader('F-F_Research_Data_Factors','famafrench', start, end)[0][['RF']].div(100)
rf.index = rf.index.to_timestamp(how='end').normalize()

# CPI data from FRED
cpi = web.DataReader('CPIAUCSL', 'fred', start, end)

# Calculate monthly inflation rates as percentage change
inflation_rate = cpi.pct_change().dropna()
inflation_rate.index = inflation_rate.index.to_period('M').to_timestamp(how='end').normalize()
inflation_rate.columns = ['Inflation Rate']

# Rolling Average
rolling_avg = returns.rolling(window=30).mean()
rolling_avg.columns = [name+"_ROLL_AVG" for name in rolling_avg.columns]
rolling_monthly = rolling_avg.resample('M').last()

# Creating the final df
result = pd.concat([returns_mon, monthly_std], axis=1, keys=['RETURN', 'STDEV'])
result.columns = result.columns.get_level_values(1) + '_' + result.columns.get_level_values(0).astype(str)

result = pd.merge(result, rf, how='left', left_index=True, right_index=True)
result = pd.merge(result, inflation_rate, how = 'left', left_index=True, right_index=True)
result = pd.merge(result, rolling_monthly, how = 'left', left_index=True, right_index=True)

# Removing first two rows because rolling average has errors
result = result.iloc[2:]

result.head()

"""# MLP
LSTM
"""

def test_train_split(stock):
    """
    Function that splits the data into training & testing data

    Parameters:
        Stock (str): Ticker of the stock

    Returns:
        df (dataframe): dataframe that contains stock return, stddev, roll_avg,
                        sp500 data, rf, inflation rate
        X_train (array): training features
        X_test (array): testing features
        y_train (array): training target
        y_test (array): testing target

    """

    # filtering the dataframe for input stock and merging it with the macroeconomic factors
    df = pd.concat([result.filter(like = stock),result[['SP500_RETURN','SP500_STDEV', 'RF', 'Inflation Rate']]], axis=1)

    # sorting dataframe
    df.sort_index(inplace=True)

    # # Including past historical distorts the result
    # aapl = aapl.loc[(aapl.index.year >= 2022)]

    # shift target variable
    df[f"Next_{stock}_RETURN"] = df[f"{stock}_RETURN"].shift(-1)

    # NaN in last row because of the shift
    df.dropna(inplace=True)

    # defining target and features
    features = [col for col in df.columns if col not in [f"Next_{stock}_RETURN"]]
    target = f"Next_{stock}_RETURN"

    X = df[features]
    y = df[target]

    # normalizing and splitting the data
    # scaler = StandardScaler()
    scaler = MinMaxScaler()

    # 80% training, 20% testing
    train_size = int(len(X) * 0.8)
    X_train, X_test = X[:train_size], X[train_size:]
    y_train, y_test = y[:train_size], y[train_size:]

    scaler.fit(X_train)
    X_train_scaled = scaler.transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    # reshaping because lstm needs 3d input
    X_train = X_train_scaled.reshape((X_train_scaled.shape[0], 1, X_train_scaled.shape[1]))
    X_test = X_test_scaled.reshape((X_test_scaled.shape[0], 1, X_test_scaled.shape[1]))

    return df, X_train, X_test, y_train, y_test

def lstm(X_train, X_test, y_train, y_test):
    """
    ML Model #1: LSTM model to predict stock returns

    Parameters:
        X_train (array): training features
        X_test (array): testing features
        y_train (array): training target
        y_test (array): testing target

    Returns:
        None

    """

    # defining the model
    model = Sequential([LSTM(50, return_sequences=True),  # , input_shape=(X_train.shape[1], X_train.shape[2])),
                        LSTM(50), LeakyReLU(alpha=0.01), Dense(1)])

    # compiling the model
    model.compile(optimizer="adam", loss="mse")

    # fitting the model
    model.fit(X_train, y_train, epochs=150, validation_data=(X_test, y_test))

    # next_month_pred = model.predict(X_test[-1].reshape(1, 1, X_test.shape[2]))
    # print("Predicted Return for Next Month:", next_month_pred[0][0])

    # getting predictions
    y_pred = model.predict(X_test)
    y_test_copy = y_test.values.reshape(-1,1)

    Y = np.hstack((y_pred, y_test_copy))
    print('Side by side comparison y_pred vs y')
    print(Y)

    # plotting the data
    plt.plot(y_test, label = "actual return")
    plt.plot(pd.Series(y_pred.flatten(), index=y_test.index), label = "predictions")
    plt.legend()
    plt.xlabel('Time')
    plt.ylabel('Returns')
    plt.show()

    print('The Mean Squared Error is:', mean_squared_error(y_pred,y_test_copy))

# testing with MSFT
input_stock = input("Enter the stock ticker: ")
df, X_train, X_test, y_train, y_test = test_train_split(input_stock)

lstm(X_train, X_test, y_train, y_test)

monthly_std.to_csv('monthly_stdev.csv')

df



# preds = {}

# for stock in stock_list:
#     next_month_pred = lstm_pred(stock)

#     preds[stock] = next_month_pred

# preds

# aapl = pd.concat([result.filter(like = 'AAPL'),result[['SP500_RETURN','SP500_STDEV', 'RF', 'Inflation Rate']]], axis=1)

# # # Including past historical distorts the result
# # aapl = aapl.loc[(aapl.index.year >= 2022)]

# # To ensure that it is sequentially ordered
# aapl.sort_index(inplace=True)

# # shift target variable
# aapl["Next_AAPL_RETURN"] = aapl["AAPL_RETURN"].shift(-1)
# # NaN in last row because of the shift
# aapl.dropna(inplace=True)

# # Defining target and features
# features = [col for col in aapl.columns if col not in ["Next_AAPL_RETURN"]]
# target = "Next_AAPL_RETURN"

# X = aapl[features]
# y = aapl[target]

# # Normalize and split the data
# # scaler = StandardScaler()
# scaler = MinMaxScaler()

# train_size = int(len(X) * 0.8)
# X_train, X_test = X[:train_size], X[train_size:]
# y_train, y_test = y[:train_size], y[train_size:]

# scaler.fit(X_train)
# X_train_scaled = scaler.transform(X_train)
# X_test_scaled = scaler.transform(X_test)

# # reshaping because lstm needs 3d input
# X_train = X_train_scaled.reshape((X_train_scaled.shape[0], 1, X_train_scaled.shape[1]))
# X_test = X_test_scaled.reshape((X_test_scaled.shape[0], 1, X_test_scaled.shape[1]))

# model = Sequential([
#     LSTM(50, return_sequences=True, input_shape=(1, X_train.shape[2])),
#     LSTM(50),
#     Dense(25, activation='relu'),
#     Dense(1)
# ])

# model.compile(optimizer="adam", loss="mse")

# model.fit(X_train, y_train, epochs=100, batch_size=16, validation_data=(X_test, y_test))

# start = dt.datetime(2000, 1, 1)
# end = dt.datetime(2025, 2,2)

# returns = yf.download(stock_list, start-pd.offsets.BDay(1), end+pd.offsets.BDay(1), auto_adjust=False)['Adj Close'].pct_change().dropna()
# check_returns_mon = returns.resample(rule = 'ME').apply(lambda x: x.add(1).prod().sub(1))

# check_returns_mon

# next_month_pred = model.predict(X_test[-1].reshape(1, 1, X_test.shape[2]))
# print("Predicted AAPL Return for Next Month:", next_month_pred[0][0])

# y_pred = model.predict(X_test)
# y_test_copy = y_test.values.reshape(-1,1)

# Y = np.hstack((y_pred, y_test_copy))
# print('Side by side comparison y_pred vs y')
# print(Y)

# plt.plot(y_test, label = "actual return")
# plt.plot(pd.Series(y_pred.flatten(), index=y_test.index), label = "predictions")
# plt.legend()
# plt.xlabel('Time')
# plt.ylabel('Returns')
# plt.show()